\section{Satellite Galaxies Around a Massive Central}

In this section we will investigate the spherical distribution of satellite galaxies around a massive central galaxies, and attempt to  fit a function to simulated data. Their density distribution $n$ can be described as

\begin{equation}
    n(x) = A\left<N_{sat}\right>\left(\frac{x}{b}\right)^{a-3}\exp{\left[-\left(\frac{x}{b}\right)^{c}\right]}\label{eq:nx}
\end{equation}

Here $x$ is the radius relative to the virial radius, i.e. $x \equiv r/r_{vir}$ with $x < x_{max} = 5$. $a$, $b$ and $c$ are free parameters, $\left<N_{sat}\right>$ is the mean number of satellites per halo and $A = A(a,b,c)$ normalizes this profile such that $\int\int\int_V n(x)dV = \left<N_{sat}\right>$. In this work we will mainly look at tt the number of satellites in the infinitesimal range $\left[x, x+dx\right>$. This is given by

\begin{equation}
    N(x)dx = n(x)4\pi x^2dx\label{eq:Nx}
\end{equation}


\subsection{Maximization}\label{sec:maxi}

We start by searching for the maximum of the distribution given by equation \ref{eq:Nx}, for this we will assume $a=2.4$, $b=0.25$, $c=1.6$. $x_{max}=5$, $\left<N_{sat}\right>=100$ and $A=256/(5\pi^{3/2})$. Instead of searching for the maximum, we instead search for the minimum of $-N(x)dx$ which gives the equivalent resulting $x$. Visually inspecting the distribution (Figure \ref{fig:maximum}), we see a clear peak at $x \sim 0.5$. To be safe we set the edges of our initial bracket at $x_{min} = 0$ and $x_{max} = 5$ and then apply a bracketing algorithm to find a three-point bracket around the minimum. Then we use this bracket as input to the golden section search algorithm to find the $x$-value at the peak. We find the following brackets and minimization results:  

\lstinputlisting[caption={Results of the maximization algorithm.}, label={lst:maxi_results}]{results/maxi_results.txt}

We also show the distribution and the exact location of this peak in Figure \ref{fig:maximum}. We can see that the algorithms have perfectly discovered the maximium of this distribution.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{results/maxi.png}
    \caption{Distribution of the number of galaxies in the infinitesimal range $\left[x, x+dx\right>$ described by equation \ref{eq:Nx}. The red cross indicates the position of the maximum of this distribution discovered using the golden search algorihtm as described in Section \ref{sec:maxi}}
    \label{fig:maximum}
\end{figure}


\subsection{Data Fitting}

We import the five datafiles provided for this hand-in which contain satellite galaxies for halos in increasing mass bins ranging from $10^{11} M_{\odot}$ up to $10^{15} M_{\odot}$. Each file contains a number of halos, $N_{halo}$, and the distances of each satellite galaxy from its center massive galaxy. For each dataset we can immediately compute $\left<N_{sat}\right>$ by dividing the total number of satellite galaxies by $N_{halo}$. We bin the data in 20 bins from $x = 10^{-4}$ to $x=5$ in log-space. We set the lower limit above $x=0$ for computational reasons because equation \ref{eq:Nx} is not defined there. We opt to show the distributions in log-space to better visualize the low $x$ region. The decision for 20 bins is relatively arbitrary but appears to result in plots that are easily interpretable. We show the binned data together with $N_{halo}$ for each dataset in Figure \ref{fig:sat_data}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{results/satellite_data.png}
    \caption{Binned satellite galaxy distance data divided into five mass bins of their massive galaxy halo as indicated on the right. The total number of halos over which the satellite galaxies are distributed is also indicated on the right.}
    \label{fig:sat_data}
\end{figure}

In the next two subsections we will first describe our two approaches to fit equation \ref{eq:Nx} to these distributions. Because that equation is normalized such that the area under the curve equals $\left<N_{sat}\right>$, we divide the histograms shown in Figure \ref{fig:sat_data} by $N_{halo}$ to recreate this. In Section \ref{sec:fitresults} we present the results of our fitting algorithms applied to the data, combined with a brief statistical analysis.

\subsubsection{Chi-Squared}

These data are discrete counts, so therefore they should be fit by a Poisson distribution. However we start with an 'easy' $\chi^2$ fit with Poisson variance (i.e. $\sigma^2 = \mu$) to compare to the proper unbiased fit. This means that we want to minimize the function

\begin{equation}
    \chi^2 = \frac{\left(y_i - \mu(x_i | \boldsymbol{p})\right)}{\mu(x_i | \boldsymbol{p})}.\label{eq:chi2}
\end{equation}

Where $x_i$ and $y_i$ are the bin center and bin counts respectively. $\mu$ is a function of the parameter describing the expected value in any particular bin, which is given by

\begin{equation}
    \mu\left(x_i | \boldsymbol{p})\right) = \tilde N_i = \int_{x_i}^{x_{i+1}} N(x)dx.\label{eq:mean}
\end{equation}

We minimize Equation \ref{eq:chi2} for each dataset separately using the Levenberg-Marquardt algorithm. As a good starting guess we use the same parameters as in Section \ref{sec:maxi}, i.e. $a=2.4$, $b=0.25$, $c=1.6$. Another important thing to note is that the normalization constant $A$ is a function of these three parameters which keep changing. We therefore compute a new value for $A$ each time the parameters are shift by first integrating over $N(x)$ from $x_{min}=10^{-4}$ to $x_{max}=5$ using Romberg integration and $A=1$. We then compute $A$ by dividing $\left<N_{sat}\right>$ by the result of this integral.


\subsubsection{Poisson}

For the Poisson fit we have two options: we can either use the same binned data as is used for the $\chi^2$ fit, or we can opt to not use bins at all. In this latter case we pretend as though we did bin the data, however with bin size sufficiently small such that each bin contains either 0 or 1 objects. Doing this allows us to make better use of the data at the cost of more computational load. In the worst case, namely the first dataset, this difference is a factor on the order of $10^4$. Unfortunately while implemented, we have been unable to use this binless fitting as computing time exceeds the maximum alloted time of ten minutes. Therefore we opt to fit the data here using the same bins as in the previous section. The log-likelihood of a Poisson distribution is given as

\begin{equation}
    -\ln \mathcal{L}(\boldsymbol{p}) = -\sum_{i=0}^{N-1} \left(y_i\ln\left[\mu(x_i | \boldsymbol{p}))\right] - \mu(x_i | \boldsymbol{p})) - \ln(y_i!)\right).\label{eq:logL}
\end{equation}

When minimizing this equation we can ignore the last term, $\ln(y_i!)$ because it is independent of the parameters we are fitting and therefore constant. We are then left with only the first two terms in the sum. For the minimization we will make use of the downhill simplex method where we initialize one of the vertices of the first simplex using the same parameters as before ($a=2.4$, $b=0.25$, $c=1.6$). We initialize the other three vertices by adding one to parameter $i$ for vertex $i$.

We also attempted to implement the Quasi-Newton method to minimize the log likelihood function, however this also has proven unsuccesful due to extremely steep gradients which often led the line minimization algorithm to regions in parameter space where the likelihood function is not defined. This only led to NaN values as a result for the parameters. While not used, the code is still included at the end of this report.

\subsection{Fit Results}\label{sec:fitresults}


